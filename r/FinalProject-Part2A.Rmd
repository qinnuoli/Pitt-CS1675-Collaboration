---
title: "Final Project Spring 2022: Part 2"
author: "Qinnuo Li, Di Zhang"
date: "3/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
Under construction.

## Load Packages

```{r, load_packages}
library(tidyverse)
library(visdat)
library(corrplot)
library(caret)
```

* `tidyverse` suite of packages is used throughout the entire project.  
* `visdat` is used in Part 1 to visually check for the missing values in the data frame. 
* `corrplot` is used to create correlation plot, to visualize the relationship between continuous inputs.  
* `caret` is used to train and accesss performance of models.  

## Read Data

```{r, read_data_01}
df_all <- readr::read_csv("final_project_train.csv", col_names = TRUE)
```

# Project Part 2: Regression

# 2.1 Preprocess the continuous output

Since Part 2 is a regression task, we will temporarily remove the categorical output `outcome` from our training data set. We will come back to `outcome` when we move to Part 3 where we perform a logistical regression task.  

From Part 1 we found out the distribution of the continuous output `response` being highly skewed, and it's specifically right skewed. We will therefore perform a log-transformation on `y`, to transform the skewed data to approximate a near normal distribution.  

```{r, log_trans_y}
dfii <- df_all %>% 
  mutate(y = log(response)) %>% 
  select(region, customer, starts_with('x'), y)

dfii %>% glimpse()
```

```{r, log_trans_y_histogram}
dfii %>% 
  select(y) %>%
  tibble::rowid_to_column() %>% 
  pivot_longer(!c("rowid")) %>% 
  ggplot(mapping = aes(x = value)) +
  geom_histogram(bins = 35) +
  geom_vline(xintercept = quantile(dfii$y),
             color = 'red', size = 1, linetype = 'dashed') +
  facet_wrap(~ name, scales = "free") +
  theme(axis.text.y = element_blank())
```

We also took a look at the correlation between each variables. Based on what we saw in data exploration, we decide to preprocess the data in order to deal with the issue of correlation and reduce its impact on our models. There are two main groups that we are concerned about [WIP].

xb_04 and xb_05 are involved in a few high correlation with other variables but have correlations close to 0 with the response.

# 2.2 Train linear models

We use `lm()` to train the following different linear models:

i) A linear model with linear additive terms using categorical variables only
```{r, train_mod01, eval=TRUE}
mod01 <- lm(y ~ region + customer, data = dfii)
mod01 %>% readr::write_rds('mod01.rds')
```

ii) A linear model with linear additive terms using continuous variables only
```{r, train_mod02, eval=TRUE}
mod02 <- lm(y ~ (.), data = dfii %>% select(-region, -customer))
mod02 %>% readr::write_rds('mod02.rds')
```

iii) A linear model with linear additive terms using all categorical and continuous variables
```{r, train_mod03, eval=TRUE}
mod03 <- lm(y ~ (.), data = dfii)
mod03 %>% readr::write_rds('mod03.rds')
```

iv) A linear model that interacts `region` with continuous inputs (`customer` excluded)
```{r, train_mod04, eval=TRUE}
mod04 <- lm(y ~ region * (.), data = dfii %>% select(-customer))
mod04 %>% readr::write_rds('mod04.rds')
```

v) A linear model that interacts `customer` with continuous inputs (`region` excluded)
```{r, train_mod05, eval=TRUE}
mod05 <- lm(y ~ customer * (.), data = dfii %>% select(-region))
mod05 %>% write_rds('mod05.rds')
```

vi) A linear model with all pairwise interaction between continuous inputs (categorical inputs excluded)
```{r, train_mod06, eval=TRUE}
mod06 <- lm(y ~ (.)^2, data = dfii %>% select(-region, -customer))
mod06 %>% write_rds('mod06.rds')
```

vii) Basis 1: a linear model with natural splines of interaction between continuous inputs as basis with degree of freedom of 8
```{r, train_mod07, eval=TRUE}
mod07 <- lm(y ~ splines::ns(), data = dfii %>% select(-region, -customer))
mod07 %>% write_rds('mod07.rds')
```

viii) Basis 2: a linear model using sine function as basis (categorical inputs excluded)
```{r}
mod08 <- lm(y ~ sin(), data = dfii %>% select(-region, -customer))
```

ix) Basis of choice 3: a linear model using cosine function as basis (categorical inputs excluded)
```{r}

```

# Training Models and Initial Evaluations

# Load test data
```{r}
# [WIP]
```

# Identifying best performing model

```{r}
# Adopted from homework 8 1h)
extract_metrics <- function(mod, mod_name)
{
  broom::glance(mod) %>% mutate(mod_name = mod_name)
}
```

```{r}
# Adopted from homework 8 1h)
all_metrics <- purrr::map2_dfr(list(mod01, mod02, mod03, mod04, mod05, mod06),
                               as.character(1:6),
                               extract_metrics)
all_metrics %>% 
  select(mod_name, df, r.squared, AIC, BIC) %>% 
  pivot_longer(!c("mod_name", "df")) %>% 
  ggplot(mapping = aes(x = mod_name, y = value)) +
  geom_point(size = 5) +
  facet_wrap(~name, scales = "free_y") +
  theme_bw()
```

# Coefficient Visualization

```{r, eval=FALSE}
mod0 %>% coefplot::coefplot() +
  theme_bw() +
  theme(legend.position = 'none')
```

# Fitting Two Models

# i)

# ii)