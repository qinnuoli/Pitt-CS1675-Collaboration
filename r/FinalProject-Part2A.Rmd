---
title: "Final Project Spring 2022: Part 2"
author: "Qinnuo Li, Di Zhang"
date: "3/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
Under construction.

## Load Packages

```{r, load_packages}
library(tidyverse)
library(visdat)
library(corrplot)
library(caret)
```

* `tidyverse` suite of packages is used throughout the entire project.  
* `visdat` is used in Part 1 to visually check for the missing values in the data frame. 
* `corrplot` is used to create correlation plot, to visualize the relationship between continuous inputs.  
* `caret` is used to train and accesss performance of models.  

## Read Data

```{r, read_data_01}
df_all <- readr::read_csv("final_project_train.csv", col_names = TRUE)
```

# Project Part 2: Regression

# 2.1 Preprocess the continuous output

Since Part 2 is a regression task, we will temporarily remove the categorical output `outcome` from our training data set. We will come back to `outcome` when we move to Part 3 where we perform a logistical regression task.  

From Part 1 we found out the distribution of the continuous output `response` being highly skewed, and it's specifically right skewed. We will therefore perform a log-transformation on `y`, to transform the skewed data to approximate a near normal distribution.  

```{r, log_trans_y}
dfii <- df_all %>% 
  mutate(y = log(response)) %>% 
  select(region, customer, starts_with('x'), y)

dfii %>% glimpse()
```

```{r, log_trans_y_histogram}
dfii %>% 
  select(y) %>%
  tibble::rowid_to_column() %>% 
  pivot_longer(!c("rowid")) %>% 
  ggplot(mapping = aes(x = value)) +
  geom_histogram(bins = 35) +
  geom_vline(xintercept = quantile(dfii$y),
             color = 'red', size = 1, linetype = 'dashed') +
  facet_wrap(~ name, scales = "free") +
  theme(axis.text.y = element_blank())
```

We also took a look at the correlation between each variables. Based on what we saw in data exploration, we decide to preprocess the data in order to deal with the issue of correlation and reduce its impact on our models. There are two main groups that we are concerned about [WIP].

xb_04 and xb_05 are involved in a few high correlation with other variables but have correlations close to 0 with the response.

# 2.2 Train linear models

We use `lm()` to train the following different linear models:

i) A linear model with linear additive terms using categorical variables only
```{r, fit_mod01, eval=TRUE}
mod01 <- lm(y ~ region + customer, data = dfii)
mod01 %>% readr::write_rds('mod01.rds')
```

ii) A linear model with linear additive terms using continuous variables only
```{r, fit_mod02, eval=TRUE}
mod02 <- lm(y ~ (.), data = dfii %>% select(-region, -customer))
mod02 %>% readr::write_rds('mod02.rds')
```

iii) A linear model with linear additive terms using all categorical and continuous variables
```{r, fit_mod03, eval=TRUE}
mod03 <- lm(y ~ (.), data = dfii)
mod03 %>% readr::write_rds('mod03.rds')
```

iv) A linear model that interacts `region` with continuous inputs (`customer` excluded)
```{r, fit_mod04, eval=TRUE}
mod04 <- lm(y ~ region * (.), data = dfii %>% select(-customer))
mod04 %>% readr::write_rds('mod04.rds')
```

v) A linear model that interacts `customer` with continuous inputs (`region` excluded)
```{r, fit_mod05, eval=TRUE}
mod05 <- lm(y ~ customer * (.), data = dfii %>% select(-region))
mod05 %>% write_rds('mod05.rds')
```

vi) A linear model with all pairwise interaction between continuous inputs (categorical inputs excluded)
```{r, fit_mod06, eval=TRUE}
mod06 <- lm(y ~ (.)^2, data = dfii %>% select(-region, -customer))
mod06 %>% write_rds('mod06.rds')
```

vii) Basis choice of 1: a linear model using interaction between quadratic terms of continuous input `xw` and categorical input `customer`.

```{r, fit_mod07}
mod07 <- lm(y ~ as.factor(customer) * (I(xw_01^2) + I(xw_02)^2 + I(xw_03)^2), data = dfii %>% select(-region))
mod07 %>% write_rds('mod07.rds')
```

viii) Basis choice of 2: a linear model using sine wave of `xa`.
```{r, fit_mod08}
mod08 <- lm(y ~ sin(xa_01) + sin(xa_02) + sin(xa_03) + sin(xa_04) + sin(xa_05) + sin(xa_06) + sin(xa_07) + sin(xa_08), data = dfii)
mod08 %>% write_rds('mod08.rds')
```

ix) Basis of choice 3: a linear model using additive natural splines terms of `xs`, with degree of freedom of 2 for each.
```{r, fit_mod09}
mod09 <- lm(y ~ splines::ns(xs_01, 2) + splines::ns(xs_02, 2) + splines::ns(xs_03, 2) + 
                     splines::ns(xs_04, 2) + splines::ns(xs_05, 2) + splines::ns(xs_06, 2),
            data = dfii)
mod09 %>% write_rds('mod09.rds')
```


# Identifying best performing model

We define a wrapper function around `broom::glance()` to include names of the model with perforamnce metrics, which is applied to each linear model and compile results into a dataframe for comparison. These code chunks are adapted from homework 8.
```{r}
extract_metrics <- function(mod, mod_name)
{
  broom::glance(mod) %>% mutate(mod_name = mod_name)
}
```

```{r}
all_linear_metrics <- purrr::map2_dfr(list(mod01, mod02, mod03, mod04, mod05, mod06, mod07, mod08, mod09),
                               as.character(1:9),
                               extract_metrics)
all_linear_metrics %>% 
  select(mod_name, df, logLik, AIC, BIC, deviance) %>% 
  pivot_longer(!c("mod_name", "df")) %>% 
  ggplot(mapping = aes(x = mod_name, y = value)) +
  geom_point(size = 5) +
  facet_wrap(~name, scales = "free_y") +
  theme_bw()
```

From the visualization, we pick the regression models with the lowest AIC and BIC, low deviance and high logLik: model 2, 3, 4. We chose model 2 for its low BIC despite a rather high deviance and low logLik.

# Coefficient Visualization
We plot the coefficient summaries for the top 3 models.

```{r}
mod02 %>% coefplot::coefplot() +
  theme_bw() +
  theme(legend.position = 'none')
mod03 %>% coefplot::coefplot() +
  theme_bw() +
  theme(legend.position = 'none')
mod04 %>% coefplot::coefplot() +
  theme_bw() +
  theme(legend.position = 'none')
```

From the visualization, we determine that out of the top three models, most of the predictors in model 4 overlap with 0 For the other two models, `xn_8`, `xb_8`, `xb_7` and `xb_4` are significant predictors in both. Additionally, in model 3, Consumer K, Q as well as both regions appear to be significant predictors.

