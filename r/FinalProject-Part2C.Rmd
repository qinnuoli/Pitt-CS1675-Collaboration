---
title: 'Final Project Spring 2022: Part 2C'
author: "Di Zhang"
date: '2022-04-27'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Packages

```{r, load_packages}
library(tidyverse)
```

## Read and Prep Data

```{r, read_data_01}
df_all <- readr::read_csv("final_project_train.csv", col_names = TRUE)
dfii <- df_all %>% 
  mutate(y = log(response)) %>% 
  select(region, customer, starts_with('x'), y)
```

## Load models
```{r, load_models}
lm_mod03 <- readr::read_rds('lm_mod03.rds')
lm_mod04 <- readr::read_rds('lm_mod04.rds')

laplace_lm_mod03 <- readr::read_rds('laplace_lm_mod03.rds')
laplace_lm_mod04 <- readr::read_rds('laplace_lm_mod04.rds')
```

## Regenerate model matrices
```{r, mod_mat}
Xmat_lm_mod03 <- model.matrix(y ~ (.), data = dfii)
Xmat_lm_mod04 <- model.matrix(y ~ region * (.), data = dfii %>% select(-customer))
```

# Project Part 2: Regression

# 2.7 Model Prediction and Trends
Define function for generating posterior samples.
```{r, generate_lm_post_samples_func, eval=TRUE}
generate_lm_post_samples <- function(mvn_result, length_beta, num_samples)
{
  MASS::mvrnorm(n = num_samples,
                mu = mvn_result$mode,
                Sigma = mvn_result$var_matrix) %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    purrr::set_names(c(sprintf("beta_%02d", 0:(length_beta-1)), "varphi")) %>% 
    mutate(sigma = exp(varphi))
}
```

Define function for generating posterior mean trend/response predictions.
```{r, post_lm_pred_samples_func, eval=TRUE}
post_lm_pred_samples <- function(Xnew, Bmat, sigma_vector)
{
  # number of new prediction locations
  M <- nrow(Xnew)
  # number of posterior samples
  S <- nrow(Bmat)
  
  # matrix of linear predictors
  Umat <- Xnew %*% t(Bmat)
  
  # assmeble matrix of sigma samples, set the number of rows
  Rmat <- matrix(rep(sigma_vector, M), M, byrow = TRUE)
  
  # generate standard normal and assemble into matrix
  # set the number of rows
  Zmat <- matrix(rnorm(M*S), M, byrow = TRUE)
  
  # calculate the random observation predictions
  Ymat <- Umat + Rmat * Zmat
  
  # package together
  list(Umat = Umat, Ymat = Ymat)
}
```

Define warpper for calling `post_lm_pred_samples()`
```{r, make_post_lm_pred_func, eval=TRUE}
make_post_lm_pred <- function(Xnew, post)
{
  Bmat <- post %>% select(starts_with("beta_")) %>% as.matrix()
  
  sigma_vector <- post %>% pull(sigma)
  
  post_lm_pred_samples(Xnew, Bmat, sigma_vector)
}
```

Define summary function for posterior prediction of the linear model.
```{r, summarize_lm_pred_from_laplace_func, eval=TRUE}
summarize_lm_pred_from_laplace <- function(mvn_result, Xtest, num_samples)
{
  # generate posterior samples of the beta parameters
  post <- generate_lm_post_samples(mvn_result, ncol(Xtest), num_samples)
  
  # make posterior predictions on the test set
  pred_test <- make_post_lm_pred(Xtest, post)
  
  # calculate summary statistics on the predicted mean and response
  # summarize over the posterior samples
  
  # posterior mean, should you summarize along rows (rowMeans) or 
  # summarize down columns (colMeans) ???
  mu_avg <- rowMeans(pred_test$Umat)
  y_avg <- rowMeans(pred_test$Ymat)
  
  # posterior quantiles for the middle 95% uncertainty intervals
  mu_lwr <- apply(pred_test$Umat, 1, stats::quantile, probs = 0.025)
  mu_upr <- apply(pred_test$Umat, 1, stats::quantile, probs = 0.975)
  y_lwr <- apply(pred_test$Ymat, 1, stats::quantile, probs = 0.025)
  y_upr <- apply(pred_test$Ymat, 1, stats::quantile, probs = 0.975)
  
  # book keeping
  tibble::tibble(
    mu_avg = mu_avg,
    mu_lwr = mu_lwr,
    mu_upr = mu_upr,
    y_avg = y_avg,
    y_lwr = y_lwr,
    y_upr = y_upr
  ) %>% 
    tibble::rowid_to_column("pred_id")
}
```

We generate posterior prediction summaries for both models.
```{r, generate_posterior_prediction_summary}
post_pred_summary_viz_lm_mod03 <- summarize_lm_pred_from_laplace(laplace_lm_mod03, Xmat_lm_mod03, 2500)
post_pred_summary_viz_lm_mod04 <- summarize_lm_pred_from_laplace(laplace_lm_mod04, Xmat_lm_mod04, 2500)
```

We will visualize prediction trends with respect to the most significant predictors as identified in iiA and iiB. For our best linear regression model 3, the most significant continuous predictor would be `xb_04`.
```{r, post_pred_mod03, eval=TRUE}
post_pred_summary_viz_lm_mod03 %>% 
  left_join(viz_grid %>% tibble::rowid_to_column("pred_id"),
            by = 'pred_id') %>% 
  ggplot(mapping = aes(x = xb_04)) +
  geom_ribbon(mapping = aes(ymin = mu_lwr,
                            ymax = mu_upr),
              fill = 'darkorange') +
  geom_line(mapping = aes(y = mu_avg),
            color = 'black') +
  coord_cartesian(ylim = c(-7, 7)) +
  labs(y = 'y') +
  facet_wrap(. ~ dfii$region)
  theme_bw()
```

Predictions fluctuates very rigidly [WIP. Will get back to this.]

```{r}
post_pred_summary_viz_lm_mod03 %>% 
  left_join(viz_grid %>% tibble::rowid_to_column("pred_id"),
            by = 'pred_id') %>% 
  ggplot(mapping = aes(x = xb_04)) +
  geom_ribbon(mapping = aes(ymin = mu_lwr,
                            ymax = mu_upr),
              fill = 'darkorange') +
  geom_line(mapping = aes(y = mu_avg),
            color = 'black') +
  coord_cartesian(ylim = c(-7, 7)) +
  labs(y = 'y') +
  facet_wrap(. ~ dfii$region)
  theme_bw()
```

